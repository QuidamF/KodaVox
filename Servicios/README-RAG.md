# RAG Qwen Starter

Este proyecto es una implementaciÃ³n de un sistema de **GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG)**, diseÃ±ado para ejecutarse localmente. Permite hacer preguntas en lenguaje natural sobre una base de documentos privados y obtener respuestas generadas por un Modelo de Lenguaje Grande (LLM) sin que tus datos salgan de tu mÃ¡quina.

## Puntos Clave
- **Privacidad:** Todo el proceso, desde el almacenamiento de documentos hasta la generaciÃ³n de respuestas, se ejecuta en contenedores Docker locales.
- **Modularidad:** Utiliza servicios desacoplados (FastAPI, Qdrant, Ollama) orquestados con `docker-compose`.
- **Extensibilidad:** FÃ¡cil de adaptar para soportar nuevos tipos de documentos o diferentes modelos de lenguaje.
- **FÃ¡cil de usar:** Con Docker, la configuraciÃ³n y ejecuciÃ³n se simplifican a unos pocos comandos.

---

## Diagrama de Funcionamiento

El sistema se compone de dos flujos principales: la ingesta de datos y la resoluciÃ³n de consultas.

### 1. Flujo de Ingesta de Datos
```mermaid
flowchart TD
    A[ðŸ“„ Documentos en /data] --> B{SCRIPT ingest.py};
    B --> C[ðŸ”¡ Extrae y divide el texto];
    C --> D[ðŸ§  Crea Embeddings con SentenceTransformers];
    D --> E[ðŸ’¾ Almacena en Qdrant];
```

### 2. Flujo de Consulta y Respuesta
```mermaid
flowchart TD
    subgraph Usuario
        A[ðŸ‘¨â€ðŸ’» Usuario]
    end

    subgraph "Servidor Local"
        B[ðŸŒ API FastAPI]
        C[ðŸ§  Embedding de la consulta]
        D[ðŸ” BÃºsqueda de Similitud en Qdrant]
        E[ðŸ“ Prompt Aumentado]
        F[ðŸ¤– LLM Local con Ollama]
        G[ðŸ’¾ Vector DB (Qdrant)]
    end

    A -- 1. EnvÃ­a Pregunta --> B;
    B -- 2. Genera Embedding --> C;
    C -- 3. Busca Contexto --> G;
    G -- 4. Devuelve Contexto --> D;
    D -- 5. Ensambla Prompt --> E;
    E -- 6. EnvÃ­a a LLM --> F;
    F -- 7. Genera Respuesta --> B;
    B -- 8. Devuelve Respuesta --> A;

```

---

## CaracterÃ­sticas Principales

- **Backend con FastAPI:** Una API robusta y rÃ¡pida para gestionar las solicitudes.
- **Vector Database con Qdrant:** Almacenamiento y bÃºsqueda de vectores de alta eficiencia.
- **LLM Local con Ollama:** Soporte para ejecutar modelos de lenguaje como Mistral, Llama, etc., de forma local, con aceleraciÃ³n de GPU configurada.
- **Ingesta de Documentos:** Scripts para procesar archivos (actualmente `.md`, pero extensible a `.pdf`, `.docx` etc.) y poblarlos en la base de datos de vectores.
- **OrquestaciÃ³n con Docker Compose:** DefiniciÃ³n clara de los servicios, redes y volÃºmenes para una fÃ¡cil gestiÃ³n.

---

## Requisitos y Sistema Recomendado

### Software
- **Sistema Operativo:** Linux, macOS, o Windows (con WSL2 para un mejor rendimiento de Docker).
- **Docker:** VersiÃ³n 20.10 o superior.
- **Docker Compose:** VersiÃ³n 1.29 o superior.
- **Git:** Para clonar el repositorio.

### Hardware (Recomendado)
- **CPU:** 8 nÃºcleos o mÃ¡s.
- **RAM:** 16 GB o mÃ¡s (el LLM consume una cantidad significativa de memoria).
- **GPU:** Una GPU NVIDIA con al menos 8 GB de VRAM es **altamente recomendada** para una inferencia rÃ¡pida del LLM. Es necesario tener los drivers de NVIDIA y el `nvidia-container-toolkit` instalados.
- **Almacenamiento:** Al menos 20 GB de espacio libre para los modelos, imÃ¡genes de Docker y la base de datos de vectores.

---

## InstalaciÃ³n y EjecuciÃ³n

1.  **Clona el repositorio:**
    ```bash
    git clone <URL-DEL-REPOSITORIO>
    cd rag-qwen-starter
    ```

2.  **AÃ±ade tus documentos:**
    Coloca los archivos que deseas consultar en el directorio `data/`.

3.  **Levanta los servicios:**
    Este comando descargarÃ¡ las imÃ¡genes necesarias, construirÃ¡ el contenedor de la aplicaciÃ³n y ejecutarÃ¡ todo en segundo plano.
    ```bash
    docker-compose up -d
    ```

4.  **Ejecuta la ingesta de datos:**
    Este comando ejecuta el script `ingest.py` dentro del contenedor de la API para procesar tus documentos y almacenarlos en Qdrant.
    ```bash
    docker-compose exec fastapi python ingest.py
    ```

5.  **Realiza una consulta:**
    Una vez que la ingesta ha terminado, puedes hacer preguntas a la API usando `curl` o cualquier cliente HTTP.
    ```bash
    curl -X POST http://localhost:8000/ask \
    -H "Content-Type: application/json" \
    -d '{"query": "resume la trama de la pelÃ­cula WALL-E"}'
    ```

    DeberÃ­as recibir una respuesta en formato JSON con la contestaciÃ³n del modelo.

---

## Estructura del Proyecto

```
.
â”œâ”€â”€ app/                  # Contiene la aplicaciÃ³n FastAPI y el script de ingesta.
â”‚   â”œâ”€â”€ Dockerfile        # Define el contenedor de la aplicaciÃ³n.
â”‚   â”œâ”€â”€ ingest.py         # Script para procesar y almacenar documentos.
â”‚   â”œâ”€â”€ main.py           # LÃ³gica principal de la API FastAPI.
â”‚   â””â”€â”€ requirements.txt  # Dependencias de Python.
â”œâ”€â”€ data/                 # Directorio para tus documentos fuente.
â”œâ”€â”€ ollama/               # Volumen persistente para los modelos de Ollama.
â”œâ”€â”€ qdrant_storage/       # Volumen persistente para la base de datos de Qdrant.
â””â”€â”€ docker-compose.yml    # Orquesta todos los servicios.
```
